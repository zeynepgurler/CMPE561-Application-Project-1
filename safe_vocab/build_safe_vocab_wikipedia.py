"""
Build a Turkish "safe vocabulary" from Wikipedia text.

Outputs:
    - safe_vocab.txt      : one token per line (used as a set in your code)
    - wiki_freq.json      : token -> frequency dict (optional extra signal)

Your existing code does:
    with open("safe_vocab.txt") as f:
        safe_vocab = {line.strip() for line in f}

    with open("normalization_resources/boun_freq.json") as f:
        boun_freq = json.load(f)

So this script is compatible: it only creates safe_vocab.txt + a separate freq JSON.
"""

import regex as re
from collections import Counter
from pathlib import Path
from typing import List, Iterable
import json

# -------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------

# Folder that contains your Wikipedia text pages
# (generated by your corpus builder that calls page.text)
CORPUS_DIR = "data/wiki_text"

# Minimum frequency threshold for a token to be included
MIN_FREQ = 5

# How many most frequent tokens to check with Zemberek
MAX_ZEMBEREK_TOKENS = 200_000

# Outputs
SAFE_VOCAB_TXT = "safe_vocab.txt"
WIKI_FREQ_JSON = "wiki_freq.json"

# Enable/disable Zemberek filtering
USE_ZEMBEREK = False

# -------------------------------------------------------------
# (Optional) Zemberek integration
# -------------------------------------------------------------
if USE_ZEMBEREK:
    # Adjust import according to your project structure
    from zemberek.start_zemberek import ZemberekAnalyzer

    print("[INFO] Initializing Zemberek...")
    morph = ZemberekAnalyzer(jar_path="zemberek/zemberek-full.jar")

    def zemberek_valid(token: str) -> bool:
        """
        Return True if Zemberek recognizes 'token' as a valid word.
        Adjust method name according to your wrapper (analyze / analyze_word).
        """
        analyses = morph.analyze(token)  # or morph.analyze_word(token)
        return bool(analyses)
else:
    def zemberek_valid(token: str) -> bool:
        return True  # accept everything if Zemberek is disabled


# -------------------------------------------------------------
# Helpers
# -------------------------------------------------------------
def iter_text_files(folder: str) -> Iterable[Path]:
    """
    Yield all .txt files recursively under folder.
    """
    root = Path(folder)
    for path in root.rglob("*.txt"):
        if path.is_file():
            yield path


def tokenize(text: str) -> List[str]:
    """
    Very simple tokenizer: Unicode letter sequences.
    You can replace this with your own tokenizer later.
    """
    return re.findall(r"\p{L}+", text, flags=re.UNICODE)


# -------------------------------------------------------------
# 1) Build frequency dictionary
# -------------------------------------------------------------
def build_frequency_dict(corpus_dir: str) -> Counter:
    freq = Counter()

    for filepath in iter_text_files(corpus_dir):
        print(f"[INFO] Reading file: {filepath}")
        with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                tokens = tokenize(line)
                tokens = [t.lower() for t in tokens if t]
                freq.update(tokens)

    print(f"[INFO] Total distinct tokens (raw): {len(freq)}")
    return freq


# -------------------------------------------------------------
# 2) Basic token filtering
# -------------------------------------------------------------
def basic_token_valid(token: str) -> bool:
    """
    Basic sanity checks before Zemberek:
        - length in a reasonable range
        - no digits
    """
    if len(token) < 2:
        return False
    if len(token) > 40:
        return False
    if token.isdigit():
        return False
    if any(ch.isdigit() for ch in token):
        return False
    return True


def apply_basic_filters(freq: Counter, min_freq: int) -> Counter:
    filtered = Counter()
    for token, count in freq.items():
        if count < min_freq:
            continue
        if not basic_token_valid(token):
            continue
        filtered[token] = count

    print(f"[INFO] Tokens after basic filters: {len(filtered)}")
    return filtered


# -------------------------------------------------------------
# 3) Zemberek filtering (optional)
# -------------------------------------------------------------
def apply_zemberek_filter(freq: Counter, max_tokens: int) -> Counter:
    if not USE_ZEMBEREK:
        print("[INFO] Zemberek disabled â€” skipping.")
        return freq

    print("[INFO] Applying Zemberek filtering...")
    filtered = Counter()

    items = freq.most_common(max_tokens)
    checked = 0
    accepted = 0

    for token, count in items:
        checked += 1
        if zemberek_valid(token):
            filtered[token] = count
            accepted += 1

        if checked % 10_000 == 0:
            print(f"[INFO] Zemberek checked={checked}, accepted={accepted}")

    print(f"[INFO] Tokens accepted by Zemberek: {len(filtered)}")
    return filtered


# -------------------------------------------------------------
# 4) Save safe_vocab.txt + wiki_freq.json
# -------------------------------------------------------------
def save_safe_vocab_and_freq(freq: Counter, vocab_path: str, freq_json_path: str):
    """
    - vocab_path: one token per line (for your current safe_vocab usage)
    - freq_json_path: JSON dict token -> frequency
    """
    # Sort by frequency desc, then alphabetically
    items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))

    # Save safe_vocab.txt
    with open(vocab_path, "w", encoding="utf-8") as f:
        for token, _ in items:
            f.write(token + "\n")

    # Save wiki_freq.json
    freq_dict = {token: count for token, count in items}
    with open(freq_json_path, "w", encoding="utf-8") as f:
        json.dump(freq_dict, f, ensure_ascii=False)

    print(f"[INFO] Saved {len(items)} tokens to {vocab_path}")
    print(f"[INFO] Saved frequencies to {freq_json_path}")


# -------------------------------------------------------------
# Main pipeline
# -------------------------------------------------------------
def build_safe_vocab():
    print("[INFO] Building frequency dictionary from corpus...")
    freq = build_frequency_dict(CORPUS_DIR)

    print("[INFO] Applying basic token filters...")
    freq = apply_basic_filters(freq, MIN_FREQ)

    print("[INFO] Applying Zemberek filter (if enabled)...")
    freq = apply_zemberek_filter(freq, MAX_ZEMBEREK_TOKENS)

    print("[INFO] Saving safe_vocab and wiki_freq...")
    save_safe_vocab_and_freq(freq, SAFE_VOCAB_TXT, WIKI_FREQ_JSON)

    print("[INFO] SAFE_VOCAB DONE.")


if __name__ == "__main__":
    build_safe_vocab()
